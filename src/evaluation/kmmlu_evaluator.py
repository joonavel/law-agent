#!/usr/bin/env python3
"""
KMMLU í‰ê°€ê¸°
Criminal-Law ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ Agent System í‰ê°€
"""

import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
from tqdm import tqdm
import sys
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datasets import load_dataset
from src.agent.workflow import create_parent_graph
from openai import OpenAI
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# OpenAI Batch APIìš© í”„ë¡¬í”„íŠ¸ ì •ì˜
PARSING_TEMPLATE = """**Problem Core:**
{problem_core}

**Relevant Legal Provisions:**
{relevant_legal_provisions}

**Solution Points:**
{solution_points}
"""

SYSTEM_PROMPT = """You are an expert in Korean criminal law. Analyze the given multiple-choice question about Korean criminal law using the provided context. 

Instructions:
- Carefully read the question, all four options (A, B, C, D), and the context.
- Apply Korean criminal law principles to determine the correct answer
- Consider legal precedents, statutory provisions, and established legal interpretations
- Do not provide explanations or reasoning process
- Respond with only the letter of the correct answer (A, B, C, or D) AND CHOOSE only one answer even if the context indicates multiple answers
- If the context imply that there is no proper answer, Answer with your Knowledge of Korean criminal law.
- If you cannot find the correct answer or it is not valid question, respond with "IDK"
"""

USER_PROMPT = """Question: {question}

Options:
A) {option_a}
B) {option_b}
C) {option_c}
D) {option_d}

Context:
{context}

Answer:"""


class KMMLUEvaluator:
    """KMMLU Criminal-Law í‰ê°€ê¸°"""
    
    def __init__(self, batch_size: int = 5, sleep_time: int = 5):
        """
        í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 5)
            sleep_time: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 5)
        """
        self.batch_size = batch_size
        self.sleep_time = sleep_time
        self.dataset = None
        self.graph = None
        self.batch_results = []
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.data_dir = Path(project_root) / "data" / "batch"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°°ì¹˜ ê´€ë ¨ íŒŒì¼ ê²½ë¡œ
        self.input_batch_file = self.data_dir / "input_batch.jsonl"
        self.output_batch_file = self.data_dir / "output_batch.jsonl"
        self.input_id_file = self.data_dir / "input_id.txt"
        self.output_id_file = self.data_dir / "output_id.txt"
        
    def load_dataset(self):
        """KMMLU Criminal-Law ë°ì´í„°ì…‹ ë¡œë“œ"""
        logger.info("KMMLU Criminal-Law ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        try:
            self.dataset = load_dataset("HAERAE-HUB/KMMLU", "Criminal-Law", split="test")
            logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.dataset)}ê°œ ë¬¸ì œ")
            return True
        except Exception as e:
            logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def initialize_graph(self):
        """ParentGraph ì´ˆê¸°í™”"""
        logger.info("ParentGraph ì´ˆê¸°í™” ì¤‘...")
        try:
            self.graph = create_parent_graph()
            logger.info("ParentGraph ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ParentGraph ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def prepare_batch_inputs(self, batch_data: List[Dict]) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        
        Args:
            batch_data: ë°°ì¹˜ ë°ì´í„°
            
        Returns:
            Graph ì…ë ¥ í˜•ì‹ì˜ ë°°ì¹˜ ë°ì´í„°
        """
        batch_inputs = []
        
        for i, item in enumerate(batch_data):
            # ì„ íƒì§€ êµ¬ì„±
            options = [
                item.get('A', ''),
                item.get('B', ''),
                item.get('C', ''),
                item.get('D', '')
            ]
            
            # Graph ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            graph_input = {
                "user_question": item.get('question', ''),
                "user_options": options,
                "thread_id": f"kmmlu_test_{i}"
            }
            
            batch_inputs.append(graph_input)
        
        return batch_inputs
    
    def evaluate_batch(self, start_idx: int = 0, end_idx: int = None) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê°€ ì‹¤í–‰
        
        Args:
            start_idx: ì‹œì‘ ì¸ë±ìŠ¤
            end_idx: ì¢…ë£Œ ì¸ë±ìŠ¤ (Noneì´ë©´ ì „ì²´ ë°ì´í„°ì…‹)
            
        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.dataset:
            logger.error("ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        if not self.graph:
            logger.error("ParentGraphê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # í‰ê°€ ë²”ìœ„ ì„¤ì •
        if end_idx is None:
            end_idx = len(self.dataset)
        
        logger.info(f"ë°°ì¹˜ í‰ê°€ ì‹œì‘: ì¸ë±ìŠ¤ {start_idx}~{end_idx} (ì´ {end_idx - start_idx}ê°œ ë¬¸ì œ)")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}, ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„: {self.sleep_time}ì´ˆ")
        
        self.batch_results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for idx in tqdm(range(start_idx, end_idx, self.batch_size), desc="KMMLU í‰ê°€ ì§„í–‰"):
            batch_end_idx = min(idx + self.batch_size, end_idx)
            
            # í˜„ì¬ ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_data = [self.dataset[i] for i in range(idx, batch_end_idx)]
            
            logger.info(f"ë°°ì¹˜ {idx//self.batch_size + 1} ì²˜ë¦¬ ì¤‘: ì¸ë±ìŠ¤ {idx}~{batch_end_idx-1}")
            
            try:
                # ë°°ì¹˜ ì…ë ¥ ì¤€ë¹„
                batch_inputs = self.prepare_batch_inputs(batch_data)
                
                # ì‹¤ì œ ì¸ë±ìŠ¤ë¡œ thread_id ì¬ì„¤ì •
                for i, batch_input in enumerate(batch_inputs):
                    batch_input["thread_id"] = f"kmmlu_test_{idx + i}"
                
                # ê·¸ë˜í”„ ë°°ì¹˜ ì‹¤í–‰
                batch_result = self.graph.batch(
                    inputs=batch_inputs, 
                    config={"recursion_limit": 25}
                )
                # ê²°ê³¼ ì €ì¥ (ì›ë³¸ ë°ì´í„°ì™€ í•¨ê»˜)
                for i, (original, result) in enumerate(zip(batch_data, batch_result)):
                    combined_result = {
                        "index": idx + i,
                        "original_data": original,
                        "agent_result": result
                    }
                    self.batch_results.append(combined_result)
                
                logger.info(f"ë°°ì¹˜ {idx//self.batch_size + 1} ì™„ë£Œ: {len(batch_result)}ê°œ ê²°ê³¼")
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {idx//self.batch_size + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ë„ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
                for i, original in enumerate(batch_data):
                    failed_result = {
                        "index": idx + i,
                        "original_data": original,
                        "agent_result": None,
                        "error": str(e)
                    }
                    self.batch_results.append(failed_result)
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (API ìš”ì²­ ì œí•œ ê³ ë ¤)
            if idx + self.batch_size < end_idx:
                logger.info(f"{self.sleep_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(self.sleep_time)
        
        logger.info(f"ë°°ì¹˜ í‰ê°€ ì™„ë£Œ: ì´ {len(self.batch_results)}ê°œ ê²°ê³¼")
        return self.batch_results
    
    def run_evaluation(self, start_idx: int = 0, end_idx: int = None, test_limit: int = None) -> List[Dict[str, Any]]:
        """
        ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            start_idx: ì‹œì‘ ì¸ë±ìŠ¤
            end_idx: ì¢…ë£Œ ì¸ë±ìŠ¤ (Noneì´ë©´ ì „ì²´ ë°ì´í„°ì…‹)
            test_limit: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ê°œìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
            
        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # 1. ë°ì´í„°ì…‹ ë¡œë“œ
        if not self.load_dataset():
            return []
        
        # 2. ê·¸ë˜í”„ ì´ˆê¸°í™”
        if not self.initialize_graph():
            return []
        
        # í…ŒìŠ¤íŠ¸ ì œí•œ ì ìš©
        if test_limit is not None:
            end_idx = min(test_limit, len(self.dataset))
            logger.info(f"í…ŒìŠ¤íŠ¸ ì œí•œ ì ìš©: {end_idx}ê°œ ë°ì´í„° ì‚¬ìš©")
        
        # 3. ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
        results = self.evaluate_batch(start_idx, end_idx)
        
        return results

    def create_batch_file_from_results(self, evaluation_results: List[Dict], output_file: str = "input_batch.jsonl") -> str:
        """
        run_evaluation ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenAI Batch APIìš© íŒŒì¼ ìƒì„±
        
        Args:
            evaluation_results: run_evaluationì—ì„œ ì–»ì€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_file: ì¶œë ¥ íŒŒì¼ëª…
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
        """
        logger.info(f"í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°°ì¹˜ íŒŒì¼ ìƒì„±: {output_file}")
        
        if not evaluation_results:
            logger.error("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return ""
        
        full_path = self.data_dir / output_file
        
        with open(full_path, 'w', encoding='utf-8') as f:
            for idx, result in enumerate(evaluation_results):
                context = None
                
                # ì›ë³¸ ë°ì´í„°ì—ì„œ ì§ˆë¬¸ê³¼ ì„ íƒì§€ ê°€ì ¸ì˜¤ê¸°
                original_data = result.get('original_data', {})
                question = original_data.get('question', '')
                options = [
                    original_data.get('A', ''),
                    original_data.get('B', ''), 
                    original_data.get('C', ''),
                    original_data.get('D', '')
                ]
                
                # parent_graph ê²°ê³¼ê°€ ìœ íš¨í•œ ê²½ìš° context ìƒì„±
                try:
                    agent_result = result.get('agent_result')
                    if agent_result and agent_result.get('question_validation') and agent_result.get('result'):
                        question_validation = agent_result['question_validation']
                        if hasattr(question_validation, 'is_valid') and question_validation.is_valid:
                            
                            problem_core = agent_result['result'].problem_core
                            relevant_legal_provisions = '\n'.join([f"- {item}" for item in agent_result['result'].relevant_legal_provisions])
                            solution_points = '\n'.join([f"- {item}" for item in agent_result['result'].solution_points])
                            
                            context = PARSING_TEMPLATE.format(
                                problem_core=problem_core,
                                relevant_legal_provisions=relevant_legal_provisions,
                                solution_points=solution_points
                            )
                            logger.info(f"ë¬¸ì œ {idx+1}: Parent Graph ê²°ê³¼ ì‚¬ìš©")
                        else:
                            logger.info(f"ë¬¸ì œ {idx+1}: ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼, fallback ì‚¬ìš©")
                    else:
                        logger.info(f"ë¬¸ì œ {idx+1}: ì—ì´ì „íŠ¸ ê²°ê³¼ ì—†ìŒ, fallback ì‚¬ìš©")
                except Exception as e:
                    logger.warning(f"ë¬¸ì œ {idx+1}: context ìƒì„± ì‹¤íŒ¨ ({e}), fallback ì‚¬ìš©")
                
                # contextê°€ Noneì´ë©´ fallback ë¬¸ìì—´ ì‚¬ìš©
                if context is None:
                    context = "Answer based on your knowledge of Korean criminal law."
                
                # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
                formatted_prompt = USER_PROMPT.format(
                    question=question,
                    option_a=options[0],
                    option_b=options[1],
                    option_c=options[2],
                    option_d=options[3],
                    context=context
                )
                
                # Responses API ë°©ì‹ì˜ íƒœìŠ¤í¬ ìƒì„±
                task = {
                    "custom_id": f"q{idx+1:03d}",
                    "method": "POST",
                    "url": "/v1/responses",
                    "body": {
                        "model": "gpt-4o-mini",
                        "instructions": SYSTEM_PROMPT,
                        "input": formatted_prompt,
                        "temperature": 0,
                        "max_tool_calls": 10,
                    }
                }
                
                f.write(json.dumps(task, ensure_ascii=False) + '\n')
        
        logger.info(f"ë°°ì¹˜ íŒŒì¼ ìƒì„± ì™„ë£Œ: {full_path}")
        logger.info(f"ì´ {len(evaluation_results)}ê°œ ë¬¸ì œ ì²˜ë¦¬ë¨")
        
        return str(full_path)

    def create_batch(self, batch_file_path: str) -> Dict[str, Any]:
        """
        ë°°ì¹˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë°°ì¹˜ ì‘ì—… ìƒì„±
        
        Args:
            batch_file_path: ì—…ë¡œë“œí•  ë°°ì¹˜ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë°°ì¹˜ ì‘ì—… ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            client = OpenAI()
            
            logger.info(f"ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: {batch_file_path}")
            
            # 1. íŒŒì¼ ì—…ë¡œë“œ
            with open(batch_file_path, "rb") as file:
                batch_input_file = client.files.create(
                    file=file,
                    purpose="batch"
                )
            
            logger.info(f"ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {batch_input_file.id}")
            
            # 2. ë°°ì¹˜ ì‘ì—… ìƒì„±
            batch_job = client.batches.create(
                input_file_id=batch_input_file.id,
                endpoint="/v1/responses",
                completion_window="24h",
                metadata={
                    "description": "Korean criminal law multiple choice questions",
                    "purpose": "legal_qa_evaluation"
                }
            )
            
            logger.info(f"ë°°ì¹˜ ì‘ì—… ìƒì„± ì™„ë£Œ: {batch_job.id}")
            logger.info(f"ìƒíƒœ: {batch_job.status}")
            
            # 3. ë°°ì¹˜ IDë¥¼ input_id.txtì— ì €ì¥
            with open(self.input_id_file, 'w', encoding='utf-8') as f:
                f.write(batch_job.id)
            logger.info(f"ë°°ì¹˜ ID ì €ì¥ ì™„ë£Œ: {self.input_id_file}")
            
            # ë°°ì¹˜ ì‘ì—… ì •ë³´ ë°˜í™˜
            batch_info = {
                "batch_id": batch_job.id,
                "status": batch_job.status,
                "input_file_id": batch_input_file.id,
                "created_at": batch_job.created_at,
                "completion_window": batch_job.completion_window,
                "metadata": batch_job.metadata
            }
            
            return batch_info
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‘ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def run_full_evaluation_with_monitoring(self) -> Optional[Dict[str, Any]]:
        """
        ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ì‚¬ìš©ì ì œì‹œ ë¡œì§)
        1ë‹¨ê³„: ë°°ì¹˜ ì—…ë¡œë“œ - ìƒˆ ë°°ì¹˜ ì‘ì—… ìƒì„± â†’ ë°°ì¹˜ IDë¥¼ input_id.txtì— ì €ì¥
        2ë‹¨ê³„: í‰ê°€ ë¡œì§ - input_id.txtì˜ ë°°ì¹˜ IDë¡œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
        3ë‹¨ê³„: í‰ê°€ ì‹œì‘ ì „ ê²€ì¦ - input_id.txtì™€ output_id.txt ì¡´ì¬ & ê°’ í™•ì¸
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("ğŸš€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
        
        # 1ë‹¨ê³„: ë°°ì¹˜ ì—…ë¡œë“œ
        logger.info("1ï¸âƒ£ ë°°ì¹˜ ì—…ë¡œë“œ ë‹¨ê³„")
        logger.info("   - Parent Graphë¡œ í‰ê°€ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš© 10ê°œ ë°ì´í„°)")
        evaluation_results = self.run_evaluation(test_limit=10)
        if not evaluation_results:
            logger.error("í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨")
            return None
        
        logger.info("   - í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°°ì¹˜ íŒŒì¼ ìƒì„±")
        batch_file_path = self.create_batch_file_from_results(evaluation_results)
        if not batch_file_path:
            logger.error("ë°°ì¹˜ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
            return None
        
        logger.info("   - ë°°ì¹˜ ì‘ì—… ìƒì„± ë° ì—…ë¡œë“œ")
        batch_info = self.create_batch(batch_file_path)
        if not batch_info:
            logger.error("ë°°ì¹˜ ì‘ì—… ìƒì„± ì‹¤íŒ¨")
            return None
        
        logger.info(f"   - ë°°ì¹˜ ID ì €ì¥ ì™„ë£Œ: {batch_info['batch_id']}")
        
        # 2ë‹¨ê³„: í‰ê°€ ë¡œì§ (ëª¨ë‹ˆí„°ë§)
        logger.info("2ï¸âƒ£ í‰ê°€ ë¡œì§ ë‹¨ê³„")
        
        # input_id.txtì— ìˆëŠ” ë°°ì¹˜ IDë¡œ ëª¨ë‹ˆí„°ë§
        if self.input_id_file.exists():
            with open(self.input_id_file, 'r', encoding='utf-8') as f:
                batch_id = f.read().strip()
            
            logger.info(f"   - ë°°ì¹˜ IDë¡œ ìƒíƒœ ëª¨ë‹ˆí„°ë§: {batch_id}")
            batch_status = self.monitor_batch(batch_id)
            
            if batch_status == "completed":
                logger.info("   - ìƒíƒœ: completed â†’ output íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
                success = self.download_batch_output(batch_id)
                if success:
                    logger.info("   - ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                else:
                    logger.warning("   - ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            else:
                logger.info(f"   - ìƒíƒœ: {batch_status} â†’ ê¸°ì¡´ output_batch.jsonlë¡œ í‰ê°€ ì§„í–‰")
        else:
            logger.warning("   - input_id.txt íŒŒì¼ì´ ì—†ìŒ")
        
        # 3ë‹¨ê³„: í‰ê°€ ì‹œì‘ ì „ ê²€ì¦
        logger.info("3ï¸âƒ£ í‰ê°€ ì‹œì‘ ì „ ê²€ì¦ ë‹¨ê³„")
        
        # input_id.txtì™€ output_id.txt íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not self.input_id_file.exists():
            logger.error("   - input_id.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        if not self.output_id_file.exists():
            logger.warning("   - output_id.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (ë°°ì¹˜ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)")
            # output_id.txtê°€ ì—†ì–´ë„ í‰ê°€ëŠ” ì§„í–‰
        
        # ë°°ì¹˜ ID ê²€ì¦
        id_match = self.verify_batch_ids()
        if not id_match:
            logger.warning("   - ë°°ì¹˜ ID ë¶ˆì¼ì¹˜ ë˜ëŠ” íŒŒì¼ ì—†ìŒ")
        
        # 4ë‹¨ê³„: ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ë¡œ í‰ê°€ ì‹¤í–‰
        logger.info("4ï¸âƒ£ ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ë¡œ í‰ê°€ ì‹¤í–‰")
        results = self.evaluate_from_batch_output()
        
        return results

    def verify_batch_ids(self) -> bool:
        """
        ë°°ì¹˜ ID ê²€ì¦
        input_id.txtì™€ output_id.txtì˜ ë°°ì¹˜ IDê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        
        Returns:
            IDê°€ ì¼ì¹˜í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.input_id_file.exists():
                logger.warning("input_id.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            if not self.output_id_file.exists():
                logger.warning("output_id.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ID ì½ê¸°
            with open(self.input_id_file, 'r', encoding='utf-8') as f:
                input_id = f.read().strip()
            
            with open(self.output_id_file, 'r', encoding='utf-8') as f:
                output_id = f.read().strip()
            
            # ID ë¹„êµ
            if input_id == output_id:
                logger.info(f"âœ… ë°°ì¹˜ ID ì¼ì¹˜: {input_id}")
                return True
            else:
                logger.warning(f"âš ï¸  ë°°ì¹˜ ID ë¶ˆì¼ì¹˜ - ì…ë ¥: {input_id}, ì¶œë ¥: {output_id}")
                logger.warning("ì¶œë ¥ ê°’ì€ ì…ë ¥ ê°’ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ê²Œ ì•„ë‹ˆë¯€ë¡œ í‰ê°€ ê²°ê³¼ëŠ” ë¯¿ì„ ìˆ˜ ì—†ë‹¤")
                return False
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ID ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def monitor_batch(self, batch_id: str) -> str:
        """
        ë°°ì¹˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§
        
        Args:
            batch_id: ë°°ì¹˜ ID
            
        Returns:
            ë°°ì¹˜ ìƒíƒœ (validating, in_progress, completed, failed, etc.)
        """
        try:
            client = OpenAI()
            batch = client.batches.retrieve(batch_id)
            
            logger.info(f"ë°°ì¹˜ ìƒíƒœ: {batch.status}")
            return batch.status
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return "error"

    def download_batch_output(self, batch_id: str) -> bool:
        """
        ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        Args:
            batch_id: ë°°ì¹˜ ID
            
        Returns:
            ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            client = OpenAI()
            batch = client.batches.retrieve(batch_id)
            
            if batch.status != "completed":
                logger.error(f"ë°°ì¹˜ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒíƒœ: {batch.status}")
                return False
            
            if not batch.output_file_id:
                logger.error("ì¶œë ¥ íŒŒì¼ IDê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            file_response = client.files.content(batch.output_file_id)
            
            # íŒŒì¼ ì €ì¥
            with open(self.output_batch_file, 'wb') as f:
                f.write(file_response.content)
            
            # output_id.txtì— ë°°ì¹˜ ID ì €ì¥
            with open(self.output_id_file, 'w', encoding='utf-8') as f:
                f.write(batch_id)
            
            logger.info(f"ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {self.output_batch_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def get_questions_and_results(self, input_file: str, output_file: str):
        """
        ì‚¬ìš©ìê°€ ì œê³µí•œ ë¡œì§ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë§¤í•‘
        
        Args:
            input_file: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            questions, results ë”•ì…”ë„ˆë¦¬
        """
        questions = {}
        with open(input_file, "r", encoding='utf-8') as f:
            for line in f:
                question = json.loads(line)
                questions[question["custom_id"]] = question["body"]

        results = {}
        with open(output_file, "r", encoding='utf-8') as f:
            for line in f:
                result = json.loads(line)
                # Responses API ì‘ë‹µ í˜•íƒœì— ë§ê²Œ ìˆ˜ì • (ì‚¬ìš©ì ì œê³µ ë¡œì§ ê¸°ë°˜)
                try:
                    content = result['response']['body']['output'][0]['content'][0]['text'].strip()
                    results[result["custom_id"]] = content
                except (KeyError, IndexError, TypeError):
                    results[result["custom_id"]] = ""
        
        return questions, results

    def evaluate_results(self, ds, results):
        """
        ì‚¬ìš©ìê°€ ì œê³µí•œ í‰ê°€ ë¡œì§
        
        Args:
            ds: ë°ì´í„°ì…‹
            results: ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            correct, fails, wrong_preds
        """
        mapping_dict = {"A": 1, "B": 2, "C": 3, "D": 4}
        correct = 0
        fails = []
        wrong_preds = []
        
        for idx in range(len(ds)):
            custom_id = f"q{idx+1:03d}"
            y_true = ds[idx]['answer']
            y_pred = mapping_dict.get(results.get(custom_id, ''), 0)
            
            if y_pred:
                if y_true == y_pred:
                    correct += 1
                else:
                    wrong_preds.append(idx)
            else:
                fails.append(idx)

        print(f"correct: {correct}, fails: {len(fails)}, wrong_preds: {len(wrong_preds)}")
        print(f"accuracy: {correct / len(ds)}")

        return correct, fails, wrong_preds

    def evaluate_from_batch_output(self) -> Optional[Dict[str, Any]]:
        """
        ì‚¬ìš©ìê°€ ì œê³µí•œ ë¡œì§ì„ ê¸°ë°˜ìœ¼ë¡œ ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ í‰ê°€
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if not self.output_batch_file.exists():
                logger.error(f"ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.output_batch_file}")
                return None
            
            # input_batch.jsonl íŒŒì¼ ê²½ë¡œ
            input_file = str(self.input_batch_file)
            if not self.input_batch_file.exists():
                logger.error(f"ì…ë ¥ ë°°ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.input_batch_file}")
                return None
            
            # KMMLU ë°ì´í„°ì…‹ ë¡œë“œ
            if not self.dataset:
                if not self.load_dataset():
                    logger.error("ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
                    return None
            
            # ì‚¬ìš©ìê°€ ì œê³µí•œ ë¡œì§ ì‚¬ìš©
            questions, results = self.get_questions_and_results(input_file, str(self.output_batch_file))
            correct, fails, wrong_preds = self.evaluate_results(self.dataset, results)
            
            # ê²°ê³¼ ì •ë¦¬
            total_count = len(self.dataset)
            correct_count = correct
            accuracy = correct / total_count if total_count > 0 else 0
            
            evaluation_result = {
                'total_count': total_count,
                'correct_count': correct_count,
                'accuracy': accuracy,
                'fails': fails,
                'wrong_preds': wrong_preds,
                'questions': questions,
                'results': results
            }
            
            logger.info(f"ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ í‰ê°€ ì™„ë£Œ: {total_count}ê°œ ë¬¸ì œ, {correct_count}ê°œ ì •ë‹µ, {accuracy:.2%} ì •í™•ë„")
            return evaluation_result
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì¶œë ¥ íŒŒì¼ í‰ê°€ ì‹¤íŒ¨: {e}")
            return None

    def get_results(self) -> List[Dict[str, Any]]:
        """í‰ê°€ ê²°ê³¼ ë°˜í™˜"""
        return self.batch_results
    
    def print_summary(self):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.batch_results:
            print("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_count = len(self.batch_results)
        success_count = sum(1 for r in self.batch_results if r.get('agent_result') is not None)
        failed_count = total_count - success_count
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ KMMLU í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ğŸ“Š ì´ ë¬¸ì œ ìˆ˜: {total_count}")
        print(f"âœ… ì„±ê³µ: {success_count}")
        print(f"âŒ ì‹¤íŒ¨: {failed_count}")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count/total_count*100:.1f}%")
        
        if failed_count > 0:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ë¬¸ì œ ì¸ë±ìŠ¤:")
            failed_indices = [r['index'] for r in self.batch_results if r.get('agent_result') is None]
            print(f"   {failed_indices}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    evaluator = KMMLUEvaluator(batch_size=5, sleep_time=10)
    
    # ì œì•ˆëœ ì›Œí¬í”Œë¡œìš°ë¡œ ì „ì²´ í‰ê°€ ì‹¤í–‰
    print("ğŸš€ KMMLU í‰ê°€ ì‹œì‘ (ì „ì²´ ì›Œí¬í”Œë¡œìš°)")
    results = evaluator.run_full_evaluation_with_monitoring()
    
    if results:
        print(f"\nâœ… ì „ì²´ í‰ê°€ ì™„ë£Œ!")
        print(f"ì´ ë¬¸ì œ ìˆ˜: {results['total_count']}")
        print(f"ì •ë‹µ: {results['correct_count']}")
        print(f"ì •í™•ë„: {results['accuracy']:.2%}")
        
        # ì‹¤íŒ¨ ë° í‹€ë¦° ë‹µë³€ ìš”ì•½
        print(f"\nğŸ“Š ì‹¤íŒ¨: {len(results['fails'])}ê°œ, í‹€ë¦° ë‹µë³€: {len(results['wrong_preds'])}ê°œ")
    else:
        print("âŒ í‰ê°€ ì‹¤íŒ¨")
    
    return results


if __name__ == "__main__":
    results = main() 